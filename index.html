---
layout: page
title: Hi, I'm Zichun
subtitle: PhD student in Natural Language Processing
use-site-title: true
---

<br>Hi! I am a first-year PhD student at Language Technologies Institute (LTI), Carnegie Mellon University (CMU),
advised by <a href="http://www.cs.cmu.edu/~cx/">Prof. Chenyan Xiong</a>. My primary research interests are exploring
novel ways to efficiently train and apply large language models in sample-limited and computation-limited scenarios. I
am currently working on <strong>data valuation</strong>, hopefully derived from <strong>model preference</strong>, for a
more efficient pre-training procedure.<br><br>

Previously, I graduated from Tsinghua University in 2023 with a major in Computer Science and Technology. I was honored
to be a member of <a href="http://nlp.csai.tsinghua.edu.cn/">THUNLP</a>, advised by <a
    href="http://nlp.csai.tsinghua.edu.cn/~lzy/">Prof. Zhiyuan Liu</a>, working closely with <a
    href="https://gaotianyu.xyz/about/">Dr. Tianyu Gao</a> and <a href="https://zzy14.github.io/">Dr. Zhengyan Zhang</a>
in prompting and few-shot learning. I was a research intern at <a
    href="https://www.cs.washington.edu/research/nlp">UWNLP</a>, advised by <a
    href="https://homes.cs.washington.edu/~swang/">Prof. Sheng Wang</a>, and an intern at Baidu NLP group.<br><br>

When I am not doing research, I like to work out, play guitar, and watch movies.<br>

<hr style="height:2px;border-width:0;color:gray;background-color:gray">

<b>Updates:</b><br><br>

<ul>
    <li><i>June 2024:</i> Check out our pretraining data curation paper: <b><a
        href="https://arxiv.org/pdf/2406.06046">MATESüßë‚Äçü§ù‚Äçüßë: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models</a></b> ‚ú®</a>
    </li><br>

    <li><i>December 2023:</i> Check out our benchmarking LLMs paper: <b><a
                href="https://arxiv.org/pdf/2312.11444.pdf">An In-depth Look at Gemini's Language
                Abilities</a></b> ‚ú®</a>
    </li><br>

    <li><i>August 2023:</i> Begin my PhD at CMU üí™
    </li><br>

    <li><i>May 2023:</i> Check out our generic retrieval augmentation paper: <b><a
                href="https://arxiv.org/pdf/2305.17331.pdf">Augmentation-Adapted Retriever Improves
                Generalization of Language Models as Generic Plug-In</a></b> at ACL 2023 (oral presentation) ‚ú®</a>
    </li><br>

    <li><i>August 2022:</i> Check out our automatic prompting paper: <b><a
                href="https://arxiv.org/pdf/2209.09401.pdf">Automatic Label Sequence Generation for
                Prompting Sequence-to-sequence Models</a></b> at COLING 2022 (oral presentation) ‚ú®</a>
    </li><br>

</ul>