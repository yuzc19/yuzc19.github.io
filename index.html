---
layout: page
title: Hi, I'm Zichun
subtitle: PhD student in Natural Language Processing
use-site-title: true
---

<br>Hi! I am a third-year PhD student at Language Technologies Institute (LTI), Carnegie Mellon University (CMU),
advised by <a href="https://www.cs.cmu.edu/~cx/">Prof. Chenyan Xiong</a>. My primary research interests are: 

<ul>
  <li><em>Intelligent and efficient LLM scaling with novel pretraining data curation and synthesis methods.</em></li>
  <li><em>Data valuation and influence attribution to better capture the impact of LLM training data.</em></li>
</ul>

Previously, I graduated from Tsinghua University in 2023 with a honours degree in Computer Science and Technology. I was honored
to be a member of <a href="https://nlp.csai.tsinghua.edu.cn/">THUNLP</a>, advised by <a
    href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Prof. Zhiyuan Liu</a>, working closely with <a
    href="https://gaotianyu.xyz/about/">Dr. Tianyu Gao</a> and <a href="https://zzy14.github.io/">Dr. Zhengyan Zhang</a>
in efficient few-shot learning.<br><br>

When I am not doing research, I like to work out, play guitar, and watch movies.<br>

<hr style="height:2px;border-width:0;color:gray;background-color:gray">

<b>Updates:</b><br><br>

<ul>
    <li><i>June 2024:</i> Check out our pretraining data curation paper: <b><a
        href="https://arxiv.org/pdf/2406.06046">MATESüßë‚Äçü§ù‚Äçüßë: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models</a></b> at NeurIPS 2024 (poster) ‚ú®</a>
    </li><br>

    <li><i>December 2023:</i> Check out our benchmarking LLMs paper: <b><a
                href="https://arxiv.org/pdf/2312.11444.pdf">An In-depth Look at Gemini's Language
                Abilities</a></b> ‚ú®</a>
    </li><br>

    <li><i>August 2023:</i> Begin my PhD at CMU üí™
    </li><br>

    <li><i>May 2023:</i> Check out our generic retrieval augmentation paper: <b><a
                href="https://arxiv.org/pdf/2305.17331.pdf">Augmentation-Adapted Retriever Improves
                Generalization of Language Models as Generic Plug-In</a></b> at ACL 2023 (oral presentation) ‚ú®</a>
    </li><br>

    <li><i>August 2022:</i> Check out our automatic prompting paper: <b><a
                href="https://arxiv.org/pdf/2209.09401.pdf">Automatic Label Sequence Generation for
                Prompting Sequence-to-sequence Models</a></b> at COLING 2022 (oral presentation) ‚ú®</a>
    </li><br>

</ul>